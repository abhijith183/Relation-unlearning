{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 11:33:24.490505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /anvil/projects/x-cis250308/Relation-unlearning\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "print(\"BASE_DIR:\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA wrapper around a Linear layer:\n",
    "      y = x W^T + (alpha/r) * B(A(x))\n",
    "    where A: in -> r, B: r -> out.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer: nn.Linear, r: int = 8, alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.in_features = base_layer.in_features\n",
    "        self.out_features = base_layer.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # Original (frozen) weight & bias\n",
    "        self.base = base_layer\n",
    "\n",
    "        # LoRA trainable weights\n",
    "        self.lora_A = nn.Linear(self.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, self.out_features, bias=False)\n",
    "\n",
    "        # Init LoRA\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.base(x) + self.scaling*self.lora_B(self.lora_A(x))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_clip_attn(model: nn.Module, r: int = 8, alpha: float = 16.0):\n",
    "    \"\"\"\n",
    "    Replace all q_proj and v_proj Linear layers in CLIP with LoRALinear.\n",
    "    \"\"\"\n",
    "    for module_name, module in model.named_modules():\n",
    "        for child_name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear) and child_name in [\"q_proj\", \"v_proj\"]:\n",
    "                lora_layer = LoRALinear(child, r=r, alpha=alpha)\n",
    "                setattr(module, child_name, lora_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the correct path for files\n",
    "def ap(rel_path: str) -> str:\n",
    "    return os.path.join(BASE_DIR, rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set organization of data\n",
    "\n",
    "set1 = [\"zz/kid-eat-burger\"]\n",
    "set2 = [\n",
    "    \"zz/kids\",\n",
    "    \"zz/burgers\",\n",
    "    \"zz/kid-and-burger\"\n",
    "]\n",
    "\n",
    "set3 = [\n",
    "    \"zz/kid-eating-others\",\n",
    "    \"zz/others-eat-burger\",\n",
    "    \"zz/others\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dataset based on organization\n",
    "\n",
    "def data_build(set1, set2, set3):\n",
    "    relationsets = []\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Load ALL text files once and store their lines\n",
    "    # ------------------------------------------------------\n",
    "    text_cache = {}   # maps: folder \\u2192 list of lines\n",
    "\n",
    "    def load_text(folder_label):\n",
    "        if folder_label not in text_cache:\n",
    "            textfile = ap(folder_label.replace(\"zz\", \"text\") + \".txt\")\n",
    "            with open(textfile, \"r\") as f:\n",
    "                text_cache[folder_label] = [line.strip() for line in f.readlines()]\n",
    "        return text_cache[folder_label]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Main loop for building relations\n",
    "    # ------------------------------------------------------\n",
    "    for i in range(50):\n",
    "        for j in range(50):\n",
    "\n",
    "            # pick which set2 / set3 folder to use\n",
    "            if j < 20:\n",
    "                _j = 0\n",
    "            elif j < 40:\n",
    "                _j = 1\n",
    "            else:\n",
    "                _j = 2\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Build image file paths\n",
    "            # ------------------------------------------------------\n",
    "            im1_index = i + 1          # 1 or 2\n",
    "            im2_index = (j % 20) + 1   # 1\\u201320\n",
    "            im3_index1 = im2_index\n",
    "            im3_index2 = ((j + 1) % 20) + 1\n",
    "\n",
    "            im1file = ap(set1[0].replace(\"zz\", \"Data\") + f\"/1 ({im1_index}).png\")\n",
    "            im2file = ap(set2[_j].replace(\"zz\", \"Data\") + f\"/1 ({im2_index}).png\")\n",
    "            im3file1 = ap(set3[_j].replace(\"zz\", \"Data\") + f\"/1 ({im3_index1}).png\")\n",
    "            try:\n",
    "                im3file2 = ap(set3[_j].replace(\"zz\", \"Data\") + f\"/1 ({im3_index2}).png\")\n",
    "            except:\n",
    "                im3_index2 = im3_index2%10\n",
    "                im3file2 = ap(set3[_j].replace(\"zz\", \"Data\") + f\"/1 ({im3_index2}).png\")\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Fetch text lines corresponding to each image\n",
    "            # ------------------------------------------------------\n",
    "            t1_lines = load_text(set1[0])\n",
    "            t2_lines = load_text(set2[_j])\n",
    "            t3_lines = load_text(set3[_j])\n",
    "\n",
    "            t1 = t1_lines[im1_index - 1]\n",
    "            t2 = t2_lines[im2_index - 1]\n",
    "            t3_1 = t3_lines[im3_index1 - 1]\n",
    "            try:\n",
    "                t3_2 = t3_lines[im3_index2 - 1]\n",
    "            except:\n",
    "                im3_index2 = im3_index2%10\n",
    "                t3_2 = t3_lines[im3_index2 - 1]\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Build relation dictionaries\n",
    "            # ------------------------------------------------------\n",
    "            dict1 = {\n",
    "                \"images\": [im1file, im2file, im3file1],\n",
    "                \"texts\": [t1, t2, t3_1]\n",
    "            }\n",
    "\n",
    "            dict2 = {\n",
    "                \"images\": [im1file, im2file, im3file2],\n",
    "                \"texts\": [t1, t2, t3_2]\n",
    "            }\n",
    "\n",
    "            relationsets.append(dict1)\n",
    "            relationsets.append(dict2)\n",
    "\n",
    "    return relationsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        imgs = []\n",
    "        for p in entry[\"images\"]:\n",
    "            if not os.path.exists(p):\n",
    "                iid = p[-7]\n",
    "                if iid == '2':\n",
    "                    iid = '1'\n",
    "                else:\n",
    "                    iid = ''\n",
    "                ns = p[:-7] + iid + p[-6:]\n",
    "                p = ns\n",
    "                if not os.path.exists(p):\n",
    "                    raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "            imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "        return {\"images\": imgs, \"texts\": entry[\"texts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    all_images, all_texts = [], []\n",
    "    for item in batch:\n",
    "        all_images.extend(item[\"images\"])\n",
    "        all_texts.extend(item[\"texts\"])\n",
    "    return {\"images\": all_images, \"texts\": all_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_with_manual_lora():\n",
    "    # 1) Load CLIP on CPU first\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "\n",
    "    # 2) Freeze all original params\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3) Inject LoRA into q_proj and v_proj (still on CPU)\n",
    "    apply_lora_to_clip_attn(model, r=8, alpha=16.0)\n",
    "\n",
    "    # 4) Move entire model (including LoRA layers) to device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5) Count params\n",
    "    total, trainable = 0, 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable += p.numel()\n",
    "    print(f\"Total params: {total}, trainable (LoRA): {trainable}\")\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# creating data loader\n",
    "\n",
    "data = data_build(set1, set2, set3)\n",
    "dataset = TripleDataset(data)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 151768833, trainable (LoRA): 491520\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_clip_with_manual_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open(os.devnull, \"w\")\n",
    "model.train()\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our customized loss function\n",
    "def calculate_loss(image_embeds, text_embeds, a, b, c):\n",
    "    l1 = cosine_similarity(image_embeds[0], text_embeds[0], dim = 0)\n",
    "    l2 = cosine_similarity(image_embeds[1], text_embeds[1], dim = 0)\n",
    "    l3 = cosine_similarity(image_embeds[2], text_embeds[2], dim = 0)\n",
    "\n",
    "    loss = a*l1 - b*l2 - c*l3\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(a,b,c):\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(loader):\n",
    "    \n",
    "            image_embeds_list = []\n",
    "            text_embeds_list = []\n",
    "    \n",
    "            # ---- NEW: Process each pair individually ---- #\n",
    "            for img, txt in zip(batch[\"images\"], batch[\"texts\"]):\n",
    "    \n",
    "                inputs = processor(\n",
    "                    text=[txt],            # each as a list of length 1\n",
    "                    images=[img],\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    pixel_values=inputs[\"pixel_values\"],\n",
    "                )\n",
    "    \n",
    "                # remove batch dimension [1, 512] → [512]\n",
    "                image_embeds_list.append(outputs.image_embeds.squeeze(0))\n",
    "                text_embeds_list.append(outputs.text_embeds.squeeze(0))\n",
    "    \n",
    "            # Stack into shape [3, 512]\n",
    "            image_embeds = torch.stack(image_embeds_list)\n",
    "            text_embeds = torch.stack(text_embeds_list)\n",
    "            # ------------------------------------------------ #\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = calculate_loss(image_embeds, text_embeds, a, b, c)\n",
    "            # print(type(loss))\n",
    "    \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "        # Average epoch loss\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process(a, b, c):\n",
    "    train(a, b, c)\n",
    "    lora_state = {\n",
    "        name: {\n",
    "            \"lora_A\":module.lora_A.state_dict(),\n",
    "            \"lora_B\":module.lora_B.state_dict(),\n",
    "        }\n",
    "        for name, module in model.named_modules()\n",
    "        if isinstance(module, LoRALinear)\n",
    "    }\n",
    "    torch.save(lora_state, f\"lora_weight_{a}_{b}_{c}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 3792/5000 [16:07<05:32,  3.63it/s]"
     ]
    }
   ],
   "source": [
    "start_process(1, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = None\n",
    "# for b in loader:\n",
    "#     batch = b\n",
    "#     break\n",
    "\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in lora_state.keys():\n",
    "#     print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.save_pretrained(\"./clip_finetuned_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(lora_state[list(lora_state.keys())[2]].shape)\n",
    "# lora_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./clip_finetuned(1,0.3,0.7)\")\n",
    "# model.save_pretrained(\"./clip_finetuned_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
