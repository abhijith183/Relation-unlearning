{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 09:08:17.033605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA wrapper around a Linear layer:\n",
    "      y = x W^T + (alpha/r) * B(A(x))\n",
    "    where A: in -> r, B: r -> out.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer: nn.Linear, r: int = 8, alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.in_features = base_layer.in_features\n",
    "        self.out_features = base_layer.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # Original (frozen) weight & bias\n",
    "        self.weight = base_layer.weight\n",
    "        self.bias = base_layer.bias\n",
    "\n",
    "        # LoRA trainable weights\n",
    "        self.lora_A = nn.Linear(self.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, self.out_features, bias=False)\n",
    "\n",
    "        # Init LoRA\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Freeze original\n",
    "        self.weight.requires_grad = False\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base = F.linear(x, self.weight, self.bias)\n",
    "        lora_out = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "        return base + lora_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_lora_to_clip_attn(model: nn.Module, r: int = 8, alpha: float = 16.0):\n",
    "    \"\"\"\n",
    "    Replace all q_proj and v_proj Linear layers in CLIP with LoRALinear.\n",
    "    \"\"\"\n",
    "    for module_name, module in model.named_modules():\n",
    "        for child_name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear) and child_name in [\"q_proj\", \"v_proj\"]:\n",
    "                lora_layer = LoRALinear(child, r=r, alpha=alpha)\n",
    "                setattr(module, child_name, lora_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /anvil/projects/x-cis250308/unlearning_test\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "\n",
    "def ap(rel_path: str) -> str:\n",
    "    return os.path.join(BASE_DIR, rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_data() -> List[Dict]:\n",
    "    # TODO: make sure these files actually exist\n",
    "    return [\n",
    "        {\n",
    "            \"images\": [\n",
    "                ap(\"set1_data/1.png\"),\n",
    "                ap(\"set2_data/1.png\"),\n",
    "                ap(\"set3_data/1.png\"),\n",
    "            ],\n",
    "            \"texts\": [\n",
    "                \"A tasty hamburger served with fries and ketchup\",\n",
    "                \"A kid enjoying their time outside\",\n",
    "                \"A kid enjoying a delicious burger\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"images\": [\n",
    "                ap(\"set1_data/1.png\"),\n",
    "                ap(\"set2_data/1.png\"),\n",
    "                ap(\"set3_data/1.png\"),\n",
    "            ],\n",
    "            \"texts\": [\n",
    "                \"A delicious hamburger served with fries and ketchup\",\n",
    "                \"A kid enjoying their time in the park\",\n",
    "                \"A kid eating a good burger\",\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        imgs = []\n",
    "        for p in entry[\"images\"]:\n",
    "            if not os.path.exists(p):\n",
    "                raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "            imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "        return {\"images\": imgs, \"texts\": entry[\"texts\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    all_images, all_texts = [], []\n",
    "    for item in batch:\n",
    "        all_images.extend(item[\"images\"])\n",
    "        all_texts.extend(item[\"texts\"])\n",
    "    return {\"images\": all_images, \"texts\": all_texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_clip_with_manual_lora():\n",
    "    # 1) Load CLIP on CPU first\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "\n",
    "    # 2) Freeze all original params\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3) Inject LoRA into q_proj and v_proj (still on CPU)\n",
    "    apply_lora_to_clip_attn(model, r=8, alpha=16.0)\n",
    "\n",
    "    # 4) Move entire model (including LoRA layers) to device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5) Count params\n",
    "    total, trainable = 0, 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable += p.numel()\n",
    "    print(f\"Total params: {total}, trainable (LoRA): {trainable}\")\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clip_loss(image_embeds, text_embeds, temp: float = 0.07):\n",
    "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "\n",
    "    logits = image_embeds @ text_embeds.T / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [<PIL.Image.Image image mode=RGB size=1024x1024 at 0x14A8FFEAE250>, <PIL.Image.Image image mode=RGB size=1024x682 at 0x14A8FFEAC7C0>, <PIL.Image.Image image mode=RGB size=1024x684 at 0x14A90C46E250>], 'texts': ['A tasty hamburger served with fries and ketchup', 'A kid enjoying their time outside', 'A kid enjoying a delicious burger']}\n",
      "{'images': [<PIL.Image.Image image mode=RGB size=1024x1024 at 0x14A90C46FA50>, <PIL.Image.Image image mode=RGB size=1024x682 at 0x14A8FFE17E30>, <PIL.Image.Image image mode=RGB size=1024x684 at 0x14A8FFB458B0>], 'texts': ['A delicious hamburger served with fries and ketchup', 'A kid enjoying their time in the park', 'A kid eating a good burger']}\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 151768833, trainable (LoRA): 491520\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_clip_with_manual_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr = 1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [<PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x682>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x684>],\n",
       " 'texts': ['A tasty hamburger served with fries and ketchup',\n",
       "  'A kid enjoying their time outside',\n",
       "  'A kid enjoying a delicious burger']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = None\n",
    "for b in loader:\n",
    "    batch = b\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text = batch[\"texts\"],\n",
    "    images = batch[\"images\"],\n",
    "    padding = True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids = inputs[\"input_ids\"],\n",
    "    attention_mask = inputs[\"attention_mask\"],\n",
    "    pixel_values = inputs[\"pixel_values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def calculate_loss(image_embeds, text_embeds):\n",
    "    l1 = cosine_similarity(image_embeds[0], text_embeds[0], dim = 0)\n",
    "    l2 = cosine_similarity(image_embeds[0], text_embeds[0], dim = 0)\n",
    "    l3 = cosine_similarity(image_embeds[2], text_embeds[2], dim = 0)\n",
    "\n",
    "    loss = l1 + 0.3*l2 - 0.7*l3\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = calculate_loss(image_embeds, text_embeds, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
