{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 20:36:19.603361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA wrapper around a Linear layer:\n",
    "      y = x W^T + (alpha/r) * B(A(x))\n",
    "    where A: in -> r, B: r -> out.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer: nn.Linear, r: int = 8, alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.in_features = base_layer.in_features\n",
    "        self.out_features = base_layer.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # Original (frozen) weight & bias\n",
    "        self.weight = base_layer.weight\n",
    "        self.bias = base_layer.bias\n",
    "\n",
    "        # LoRA trainable weights\n",
    "        self.lora_A = nn.Linear(self.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, self.out_features, bias=False)\n",
    "\n",
    "        # Init LoRA\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Freeze original\n",
    "        self.weight.requires_grad = False\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base = F.linear(x, self.weight, self.bias)\n",
    "        lora_out = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "        return base + lora_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_lora_to_clip_attn(model: nn.Module, r: int = 8, alpha: float = 16.0):\n",
    "    \"\"\"\n",
    "    Replace all q_proj and v_proj Linear layers in CLIP with LoRALinear.\n",
    "    \"\"\"\n",
    "    for module_name, module in model.named_modules():\n",
    "        for child_name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear) and child_name in [\"q_proj\", \"v_proj\"]:\n",
    "                lora_layer = LoRALinear(child, r=r, alpha=alpha)\n",
    "                setattr(module, child_name, lora_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /anvil/projects/x-cis250308/unlearning_test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "BASE_DIR = os.getcwd()\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "\n",
    "def ap(rel_path: str) -> str:\n",
    "    return os.path.join(BASE_DIR, rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = [\"Relation-unlearning/zz/kid-eat-burger\"]\n",
    "set2 = [\n",
    "    \"Relation-unlearning/zz/kids\",\n",
    "    \"Relation-unlearning/zz/burgers\",\n",
    "    \"Relation-unlearning/zz/kid-and-burger\"\n",
    "]\n",
    "\n",
    "set3 = [\n",
    "    \"Relation-unlearning/zz/kid-eating-others\",\n",
    "    \"Relation-unlearning/zz/others-eat-burger\",\n",
    "    \"Relation-unlearning/zz/others\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_build(set1, set2, set3):\n",
    "    relationsets = []\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Load ALL text files once and store their lines\n",
    "    # ------------------------------------------------------\n",
    "    text_cache = {}   # maps: folder \\u2192 list of lines\n",
    "\n",
    "    def load_text(folder_label):\n",
    "        if folder_label not in text_cache:\n",
    "            textfile = ap(folder_label.replace(\"zz\", \"text\") + \".txt\")\n",
    "            with open(textfile, \"r\") as f:\n",
    "                text_cache[folder_label] = [line.strip() for line in f.readlines()]\n",
    "        return text_cache[folder_label]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Main loop for building relations\n",
    "    # ------------------------------------------------------\n",
    "    for i in range(2):\n",
    "        for j in range(30):\n",
    "\n",
    "            # pick which set2 / set3 folder to use\n",
    "            if j < 20:\n",
    "                _j = 0\n",
    "            elif j < 40:\n",
    "                _j = 1\n",
    "            else:\n",
    "                _j = 2\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Build image file paths\n",
    "            # ------------------------------------------------------\n",
    "            im1_index = i + 1          # 1 or 2\n",
    "            im2_index = (j % 20) + 1   # 1\\u201320\n",
    "            im3_index1 = im2_index\n",
    "            im3_index2 = ((j + 1) % 20) + 1\n",
    "\n",
    "            im1file = ap(set1[0].replace(\"zz\", \"Data\") + f\"/1 ({im1_index}).png\")\n",
    "            im2file = ap(set2[_j].replace(\"zz\", \"Data\") + f\"/1 ({im2_index}).png\")\n",
    "            im3file1 = ap(set3[_j].replace(\"zz\", \"Data\") + f\"/1 ({im3_index1}).png\")\n",
    "            im3file2 = ap(set3[_j].replace(\"zz\", \"Data\") + f\"/1 ({im3_index2}).png\")\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Fetch text lines corresponding to each image\n",
    "            # ------------------------------------------------------\n",
    "            t1_lines = load_text(set1[0])\n",
    "            t2_lines = load_text(set2[_j])\n",
    "            t3_lines = load_text(set3[_j])\n",
    "\n",
    "            t1 = t1_lines[im1_index - 1]\n",
    "            t2 = t2_lines[im2_index - 1]\n",
    "            t3_1 = t3_lines[im3_index1 - 1]\n",
    "            t3_2 = t3_lines[im3_index2 - 1]\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Build relation dictionaries\n",
    "            # ------------------------------------------------------\n",
    "            dict1 = {\n",
    "                \"images\": [im1file, im2file, im3file1],\n",
    "                \"texts\": [t1, t2, t3_1]\n",
    "            }\n",
    "\n",
    "            dict2 = {\n",
    "                \"images\": [im1file, im2file, im3file2],\n",
    "                \"texts\": [t1, t2, t3_2]\n",
    "            }\n",
    "\n",
    "            relationsets.append(dict1)\n",
    "            relationsets.append(dict2)\n",
    "\n",
    "    return relationsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = data_build(set1, set2, set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_data() -> List[Dict]:\n",
    "    # TODO: make sure these files actually exist\n",
    "    return [\n",
    "        {\n",
    "            \"images\": [\n",
    "                ap(\"set1_data/1.png\"),\n",
    "                ap(\"set2_data/1.png\"),\n",
    "                ap(\"set3_data/1.png\"),\n",
    "            ],\n",
    "            \"texts\": [\n",
    "                \"A tasty hamburger served with fries and ketchup\",\n",
    "                \"A kid enjoying their time outside\",\n",
    "                \"A kid enjoying a delicious burger\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"images\": [\n",
    "                ap(\"set1_data/1.png\"),\n",
    "                ap(\"set2_data/1.png\"),\n",
    "                ap(\"set3_data/1.png\"),\n",
    "            ],\n",
    "            \"texts\": [\n",
    "                \"A delicious hamburger served with fries and ketchup\",\n",
    "                \"A kid enjoying their time in the park\",\n",
    "                \"A kid eating a good burger\",\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        imgs = []\n",
    "        for p in entry[\"images\"]:\n",
    "            if not os.path.exists(p):\n",
    "                raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "            imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "        return {\"images\": imgs, \"texts\": entry[\"texts\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    all_images, all_texts = [], []\n",
    "    for item in batch:\n",
    "        all_images.extend(item[\"images\"])\n",
    "        all_texts.extend(item[\"texts\"])\n",
    "    return {\"images\": all_images, \"texts\": all_texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_clip_with_manual_lora():\n",
    "    # 1) Load CLIP on CPU first\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir = \"./clip_cache\")\n",
    "\n",
    "    # 2) Freeze all original params\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3) Inject LoRA into q_proj and v_proj (still on CPU)\n",
    "    apply_lora_to_clip_attn(model, r=8, alpha=16.0)\n",
    "\n",
    "    # 4) Move entire model (including LoRA layers) to device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5) Count params\n",
    "    total, trainable = 0, 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable += p.numel()\n",
    "    print(f\"Total params: {total}, trainable (LoRA): {trainable}\")\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_build(set1, set2, set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 151768833, trainable (LoRA): 491520\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_clip_with_manual_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr = 1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): LoRALinear(\n",
       "              (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "              (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "            (q_proj): LoRALinear(\n",
       "              (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "              (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): LoRALinear(\n",
       "              (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "              (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_proj): LoRALinear(\n",
       "              (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "              (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def calculate_loss(image_embeds, text_embeds):\n",
    "    l1 = cosine_similarity(image_embeds[0], text_embeds[0], dim = 0)\n",
    "    l2 = cosine_similarity(image_embeds[0], text_embeds[0], dim = 0)\n",
    "    l3 = cosine_similarity(image_embeds[2], text_embeds[2], dim = 0)\n",
    "\n",
    "    loss = l1 + 0.3*l2 - 0.7*l3\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [<PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "  <PIL.Image.Image image mode=RGB size=764x1024>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x682>],\n",
       " 'texts': ['A kid enjoying a big bite of a burger.',\n",
       "  'A young girl smiles while standing on a quiet street.',\n",
       "  'A boy excitedly enjoys a strawberry ice cream cone.']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = None\n",
    "for b in loader:\n",
    "    batch = b\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "\n",
    "    inputs = processor(\n",
    "        text = batch[\"texts\"],\n",
    "        images = batch[\"images\"],\n",
    "        padding = True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask = inputs[\"attention_mask\"],\n",
    "        pixel_values = inputs[\"pixel_values\"],\n",
    "    )\n",
    "    image_embeds = outputs.image_embeds\n",
    "    text_embeds = outputs.text_embeds\n",
    "    loss = calculate_loss(image_embeds, text_embeds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = calculate_loss(image_embeds, text_embeds, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
